#!/usr/bin/env python3

import argparse
import datetime
import math
import sys
import time
import urllib.request
from collections import Counter
from pathlib import Path
from typing import List, Optional, Set

import feedparser  # type: ignore
import html2text
import pandoc
from pandoc.types import (  # pylint: disable=no-name-in-module
    Attr,
    Block,
    Format,
    Header,
    Link,
    Meta,
    MetaBlocks,
    MetaInlines,
    MetaList,
    Pandoc,
    Para,
    RawBlock,
    Space,
    Str,
)
from readability import Document  # type: ignore


def lnk(data: str, target: str) -> Link:
    return Link(NO_ATTR, [Str(data)], (f"#{target}", ""))


# https://rosettacode.org/wiki/Entropy#Python:_More_succinct_version
def entropy(data: str) -> float:
    p, lns = Counter(data), float(len(data))
    return -sum(count / lns * math.log(count / lns, 2) for count in p.values())


MIN_ARTICLE_LEN = 500
MAX_ARTICLE_ENTROPY = 5.05

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 "
    "(KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Encoding": "none",
    "Accept-Language": "en-US,en;q=0.8",
    "Connection": "keep-alive",
}

parser = argparse.ArgumentParser(description="Create PDF from RSS feeds")
parser.add_argument(
    "-o", "--output", nargs=1, action="store", type=Path, required=True, help="output file"
)
parser.add_argument(
    "-a",
    "--age",
    nargs=1,
    action="store",
    type=int,
    default=[24],
    help="Maximum age (default: %(default)s)",
)
parser.add_argument("urls", nargs="+", action="store", help="RSS URLs to convert")
args = parser.parse_args()

news = []

for url in args.urls:
    print(f"Retrieving RSS from {url}")
    doc = feedparser.parse(url)
    for entry in doc.entries:

        if "updated_parsed" in entry:
            date_parsed = entry.updated_parsed
        elif "published_parsed" in entry:
            date_parsed = entry.published_parsed
        elif "created_parsed" in entry:
            date_parsed = entry.created_parsed
        assert date_parsed is not None

        news.append(
            (
                datetime.datetime.fromtimestamp(time.mktime(date_parsed)).replace(
                    tzinfo=datetime.timezone.utc
                ),
                doc.feed.title,
                entry.title,
                entry.description if "description" in entry else None,
                entry.link.encode("ascii", "ignore").decode("ascii"),
                entry.id if "id" in entry else entry.link,
            )
        )


NO_ATTR: Attr = ("", [], [])
TOP_LINK = Link(NO_ATTR, [Str("[Top]")], ("#0", ""))

content: List[Block] = []
seen: Set[str] = set()
full: List[Block] = []

channels: Set[str] = set()
start: Optional[datetime.datetime] = None
end: Optional[datetime.datetime] = None
has_content: bool = False

for index, (date, chan, title, desc, location, eid) in enumerate(
    reversed(sorted(news, key=lambda e: e[0]))
):
    assert title is not None

    age = datetime.datetime.now().replace(tzinfo=datetime.timezone.utc) - date
    if age > datetime.timedelta(hours=args.age[0]):
        continue

    if eid in seen:
        continue

    if location is not None:
        seen.add(eid)

    print(f"Retrieving article {location}")

    request = urllib.request.Request(location, headers=HEADERS)
    try:
        with urllib.request.urlopen(request) as response:
            charset = response.headers.get_content_charset() or "utf-8"
            article = Document(response.read().decode(charset))
            summary = article.summary()
            TITLE = article.title()
            converter = html2text.HTML2Text()
            converter.ignore_images = True
            converter.bypass_tables = True
            doc_text = converter.handle(summary)
            if len(doc_text) > MIN_ARTICLE_LEN and entropy(doc_text) <= MAX_ARTICLE_ENTROPY:
                has_content = True  # pylint: disable=invalid-name
                doc = pandoc.read(doc_text)
                full.append(RawBlock(Format("tex"), "\\newpage"))
                full.append(Header(1, (f"{index}-full", [], []), [lnk(TITLE, str(index))]))
                full.append(RawBlock(Format("tex"), "\\begin{multicols}{2}"))
                full.extend(doc[1])
                full.append(RawBlock(Format("tex"), "\\end{multicols}"))
                full.append(Para([lnk("[Back]", str(index)), TOP_LINK]))
    except (urllib.error.HTTPError, urllib.error.URLError) as e:
        print(f"Error retrieving {location}: {e}")

    if has_content:
        if chan is not None:
            channels.add(chan)
        if start is None or start > date:
            start = date
        if end is None or end < date:
            end = date
        date_str = date.strftime("%a, %Y-%m-%d %H:%M")
        content.append(
            Header(1, (f"{index}", [], []), [Link(NO_ATTR, [Str(title)], (f"#{index}-full", ""))])
        )
        content.append(Para([Str(f"{chan},"), Space(), Str(f"{date_str}"), Space(), TOP_LINK]))
        if desc:
            content.append(RawBlock(Format("tex"), "\\begin{multicols}{2}"))
            content.append(Para([Str(desc)]))
            content.append(RawBlock(Format("tex"), "\\end{multicols}"))

    has_content = False  # pylint: disable=invalid-name

title_str = (
    "News"
    if start is None or end is None
    else f"{start.strftime('%a, %d.%m. %H:%M')} - {end.strftime('%a, %d.%m. %H:%M')}"
)

content += full

if not content:
    print("No content")
    sys.exit(1)

doc = Pandoc(
    Meta(
        {
            "title": MetaInlines([Str(" | ".join(channels))]),
            "author": MetaInlines([Str(title_str)]),
            "papersize": MetaInlines([Str("a5")]),
            "geometry": MetaInlines([Str("margin=5pt")]),
            "header-includes": MetaList(
                [MetaBlocks([RawBlock(Format("tex"), "\\usepackage{multicol}")])]
            ),
        }
    ),
    content,
)
pandoc.write(doc, file=args.output[0], format="pdf", options=["--pdf-engine=xelatex"])
