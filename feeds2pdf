#!/usr/bin/env python3

import argparse
import datetime
import math
import sys
import time
import urllib.request
from collections import Counter
from pathlib import Path
from typing import List, Optional, Set

import feedparser  # type: ignore
import html2text
import pandoc
from pandoc.types import (  # pylint: disable=no-name-in-module
    Attr,
    Block,
    Emph,
    Format,
    Header,
    Link,
    Meta,
    MetaBlocks,
    MetaInlines,
    MetaList,
    Pandoc,
    Para,
    RawBlock,
    Space,
    Str,
)
from readability import Document  # type: ignore


def lnk(data: str, target: str) -> Link:
    return Link(NO_ATTR, [Str(data)], (f"#{target}", ""))


def status(omit: bool, message: str, end: str = "\n", flush: bool = False) -> None:
    if not omit:
        print(message, end=end, flush=flush)


def convert(
    text: str, min_len: int = None, max_len: int = None, max_entropy: float = None
) -> List[Block]:

    # https://rosettacode.org/wiki/Entropy#Python:_More_succinct_version
    def entropy(data: str) -> float:
        p, lns = Counter(data), float(len(data))
        return -sum(count / lns * math.log(count / lns, 2) for count in p.values())

    converter = html2text.HTML2Text()
    converter.ignore_images = True
    converter.bypass_tables = True
    raw_text = converter.handle(text)

    if min_len and len(raw_text) <= min_len:
        return []

    if max_entropy and entropy(raw_text) > max_entropy:
        return []

    if max_len and len(raw_text) > max_len:
        raw_text = raw_text[0:max_len] + "â€¦"

    return pandoc.read(raw_text)[1]


MIN_DESC_LEN = 15
MAX_DESC_LEN = 200
MIN_ARTICLE_LEN = 500
MAX_ARTICLE_ENTROPY = 5.05

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 "
    "(KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Encoding": "none",
    "Accept-Language": "en-US,en;q=0.8",
    "Connection": "keep-alive",
}

parser = argparse.ArgumentParser(description="Create PDF from RSS feeds")
parser.add_argument(
    "-o", "--output", nargs=1, action="store", type=Path, required=True, help="output file"
)
parser.add_argument(
    "-a",
    "--age",
    nargs=1,
    action="store",
    type=int,
    default=[24],
    help="Maximum age (default: %(default)s)",
)
parser.add_argument(
    "-f",
    "--filter",
    nargs=1,
    action="append",
    type=str,
    help="Filter articles mating tag",
)
parser.add_argument(
    "-q",
    "--quiet",
    action="store_true",
    help="Suppress all regular output",
)
parser.add_argument("urls", nargs="+", action="store", help="RSS URLs to convert")
args = parser.parse_args()

news = []

for url in args.urls:
    status(args.quiet, f"Retrieving feed from {url}")
    doc = feedparser.parse(url)
    for entry in doc.entries:

        tags = [tag.term for tag in entry.tags] if "tags" in entry else []
        if args.filter and any(tag in args.filter[0] for tag in tags):
            continue

        if "updated_parsed" in entry:
            date_parsed = entry.updated_parsed
        elif "published_parsed" in entry:
            date_parsed = entry.published_parsed
        elif "created_parsed" in entry:
            date_parsed = entry.created_parsed
        assert date_parsed is not None

        news.append(
            (
                datetime.datetime.fromtimestamp(time.mktime(date_parsed)).replace(
                    tzinfo=datetime.timezone.utc
                ),
                doc.feed.title,
                entry.title,
                entry.description if "description" in entry else None,
                entry.link.encode("ascii", "ignore").decode("ascii"),
                entry.id if "id" in entry else entry.link,
                tags,
            )
        )


NO_ATTR: Attr = ("", [], [])
TOP_LINK = Link(NO_ATTR, [Str("[Top]")], ("#0", ""))

content: List[Block] = []
seen: Set[str] = set()
full: List[Block] = []

channels: Set[str] = set()
start_time: Optional[datetime.datetime] = None
end_time: Optional[datetime.datetime] = None
has_content: bool = False
index = 0  # pylint: disable=invalid-name

status(args.quiet, f"Fetching max. {len(news)} articles ")

for date, chan, title, desc, location, eid, tags in reversed(sorted(news, key=lambda e: e[0])):
    assert title is not None

    age = datetime.datetime.now().replace(tzinfo=datetime.timezone.utc) - date
    if args.age[0] > 0 and age > datetime.timedelta(hours=args.age[0]):
        continue

    if eid in seen:
        continue

    if location is not None:
        seen.add(eid)

    source_link = Link(NO_ATTR, [Str("[Source]")], (location, ""))
    status(args.quiet, ".", end="", flush=True)

    request = urllib.request.Request(location, headers=HEADERS)
    try:
        with urllib.request.urlopen(request) as response:
            charset = response.headers.get_content_charset() or "utf-8"
            article = Document(response.read().decode(charset), url=location)
            summary = article.summary()
            TITLE = article.title()
            summary_md = convert(summary, min_len=MIN_ARTICLE_LEN, max_entropy=MAX_ARTICLE_ENTROPY)
            if summary_md:
                has_content = True  # pylint: disable=invalid-name
                full.append(RawBlock(Format("tex"), "\\newpage"))
                full.append(Header(1, (f"{index}-full", [], []), [lnk(TITLE, str(index))]))
                full.append(RawBlock(Format("tex"), "\\begin{multicols}{2}"))
                full.extend(summary_md)
                full.append(RawBlock(Format("tex"), "\\end{multicols}"))
                full.append(Para([lnk("[Back]", str(index)), TOP_LINK, source_link]))
    except (urllib.error.HTTPError, urllib.error.URLError):
        status(args.quiet, "E", end="", flush=True)

    if has_content:
        if chan is not None:
            channels.add(chan)
        if start_time is None or start_time > date:
            start_time = date
        if end_time is None or end_time < date:
            end_time = date
        date_str = date.strftime("%a, %Y-%m-%d %H:%M")
        content.append(
            Header(1, (f"{index}", [], []), [Link(NO_ATTR, [Str(title)], (f"#{index}-full", ""))])
        )
        content.append(
            Para(
                [
                    TOP_LINK,
                    Space(),
                    source_link,
                    Space(),
                    Str(f"{chan},"),
                    Space(),
                    Str(f"{date_str}"),
                    Space(),
                    Emph([Str(tag) for tag in tags]),
                ]
            )
        )
        if desc:
            desc_md = convert(desc, min_len=MIN_DESC_LEN, max_len=MAX_DESC_LEN)
            if desc_md:
                content.append(RawBlock(Format("tex"), "\\begin{multicols}{2}"))
                content.extend(desc_md)
                content.append(RawBlock(Format("tex"), "\\end{multicols}"))
        index += 1

    has_content = False  # pylint: disable=invalid-name

status(args.quiet, " done")

title_str = (
    "News"
    if start_time is None or end_time is None
    else f"{start_time.strftime('%a, %d.%m. %H:%M')} - {end_time.strftime('%a, %d.%m. %H:%M')}"
)

content += full

if not content:
    status(args.quiet, "No content")
    sys.exit(1)

doc = Pandoc(
    Meta(
        {
            "title": MetaInlines([Str(" | ".join(channels))]),
            "author": MetaInlines([Str(title_str)]),
            "papersize": MetaInlines([Str("a5")]),
            "geometry": MetaInlines([Str("margin=5pt")]),
            "header-includes": MetaList(
                [
                    MetaBlocks([RawBlock(Format("tex"), "\\usepackage{multicol}")]),
                    MetaBlocks([RawBlock(Format("tex"), "\\setlength{\\columnseprule}{0.4pt}")]),
                    MetaBlocks([RawBlock(Format("tex"), "\\setlength{\\columnsep}{2em}")]),
                ],
            ),
        }
    ),
    content,
)
pandoc.write(doc, file=args.output[0], format="pdf", options=["--pdf-engine=xelatex"])
