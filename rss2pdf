#!/usr/bin/env python3

import argparse
import datetime
import math
import sys
import time
import urllib.request
import xml.etree.ElementTree as ET
from collections import Counter
from email.utils import parsedate
from pathlib import Path
from typing import List, Optional, Set

import html2text
import pandoc
from pandoc.types import (  # pylint: disable=no-name-in-module
    Attr,
    Block,
    Format,
    Header,
    Link,
    Meta,
    MetaBlocks,
    MetaInlines,
    MetaList,
    Pandoc,
    Para,
    RawBlock,
    Space,
    Str,
)
from readability import Document  # type: ignore


def lnk(data: str, target: str) -> Link:
    return Link(NO_ATTR, [Str(data)], (f"#{target}", ""))


# https://rosettacode.org/wiki/Entropy#Python:_More_succinct_version
def entropy(data: str) -> float:
    p, lns = Counter(data), float(len(data))
    return -sum(count / lns * math.log(count / lns, 2) for count in p.values())


MIN_ARTICLE_LEN = 500
MAX_ARTICLE_ENTROPY = 5.05

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 "
    "(KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Encoding": "none",
    "Accept-Language": "en-US,en;q=0.8",
    "Connection": "keep-alive",
}

parser = argparse.ArgumentParser(description="Create PDF from RSS feeds")
parser.add_argument(
    "-o", "--output", nargs=1, action="store", type=Path, required=True, help="output file"
)
parser.add_argument(
    "-a",
    "--age",
    nargs=1,
    action="store",
    type=int,
    default=[24],
    help="Maximum age (default: %(default)s)",
)
parser.add_argument("urls", nargs="+", action="store", help="RSS URLs to convert")
args = parser.parse_args()

news = []

for url in args.urls:
    print(f"Retrieving RSS from {url}")
    request = urllib.request.Request(url, headers=HEADERS)
    try:
        with urllib.request.urlopen(request) as response:
            tree = ET.ElementTree(ET.fromstring(response.read()))
            for channel in tree.findall("./channel"):
                chan_title = channel.find("title")
                assert chan_title is not None
                for item in channel.findall("./item"):
                    for child in item:
                        text = child.text.strip(" \n") if child.text else None
                        if child.tag == "title":
                            title = text
                        elif child.tag == "description":
                            description = text
                        elif child.tag == "link":
                            link = text
                        elif child.tag == "pubDate":
                            parsed_date = parsedate(text)
                            if parsed_date:
                                date = datetime.datetime.fromtimestamp(
                                    time.mktime(parsed_date)
                                ).replace(tzinfo=datetime.timezone.utc)
                            else:
                                assert text is not None
                                date = datetime.datetime.strptime(
                                    text, "%Y-%m-%d %H:%M:%S"
                                ).astimezone()
                        elif child.tag == "pubdate":
                            assert text is not None
                            date = datetime.datetime.strptime(
                                text, "%Y-%m-%dT%H:%M:%S.000%z"
                            ).astimezone()
                        elif child.tag == "guid":
                            guid = text
                    news.append((date, chan_title.text, title, description, link, guid or link))
    except urllib.error.HTTPError as e:
        print(f"Error connecting to {url}: {e}")


NO_ATTR: Attr = ("", [], [])
TOP_LINK = Link(NO_ATTR, [Str("[Top]")], ("#0", ""))

content: List[Block] = []
seen: Set[str] = set()
full: List[Block] = []

channels: Set[str] = set()
start: Optional[datetime.datetime] = None
end: Optional[datetime.datetime] = None
has_content: bool = False

for index, (date, chan, title, desc, link, guid) in enumerate(
    reversed(sorted(news, key=lambda e: e[0]))
):
    assert title is not None

    age = datetime.datetime.now().replace(tzinfo=datetime.timezone.utc) - date
    if age > datetime.timedelta(hours=args.age[0]):
        continue

    if guid in seen:
        continue

    if guid is not None:
        seen.add(guid)

    print(f"Retrieving article {guid}")

    assert link is not None
    request = urllib.request.Request(link, headers=HEADERS)
    try:
        with urllib.request.urlopen(request) as response:
            charset = response.headers.get_content_charset() or "utf-8"
            article = Document(response.read().decode(charset))
            summary = article.summary()
            TITLE = article.title()
            converter = html2text.HTML2Text()
            converter.ignore_images = True
            converter.bypass_tables = True
            doc_text = converter.handle(summary)
            if len(doc_text) > MIN_ARTICLE_LEN and entropy(doc_text) <= MAX_ARTICLE_ENTROPY:
                has_content = True  # pylint: disable=invalid-name
                doc = pandoc.read(doc_text)
                full.append(RawBlock(Format("tex"), "\\newpage"))
                full.append(Header(1, (f"{index}-full", [], []), [lnk(TITLE, str(index))]))
                full.append(RawBlock(Format("tex"), "\\begin{multicols}{2}"))
                full.extend(doc[1])
                full.append(RawBlock(Format("tex"), "\\end{multicols}"))
                full.append(Para([lnk("[Back]", str(index)), TOP_LINK]))
    except (urllib.error.HTTPError, urllib.error.URLError) as e:
        print(f"Error retrieving {link}: {e}")

    if has_content:
        if chan is not None:
            channels.add(chan)
        if start is None or start > date:
            start = date
        if end is None or end < date:
            end = date
        date_str = date.strftime("%a, %Y-%m-%d %H:%M")
        content.append(
            Header(1, (f"{index}", [], []), [Link(NO_ATTR, [Str(title)], (f"#{index}-full", ""))])
        )
        content.append(Para([Str(f"{chan},"), Space(), Str(f"{date_str}"), Space(), TOP_LINK]))
        if desc:
            content.append(RawBlock(Format("tex"), "\\begin{multicols}{2}"))
            content.append(Para([Str(desc)]))
            content.append(RawBlock(Format("tex"), "\\end{multicols}"))

    has_content = False  # pylint: disable=invalid-name

title_str = (
    "News"
    if start is None or end is None
    else f"{start.strftime('%a, %d.%m. %H:%M')} - {end.strftime('%a, %d.%m. %H:%M')}"
)

content += full

if not content:
    print("No content")
    sys.exit(1)

doc = Pandoc(
    Meta(
        {
            "title": MetaInlines([Str(" | ".join(channels))]),
            "author": MetaInlines([Str(title_str)]),
            "papersize": MetaInlines([Str("a5")]),
            "geometry": MetaInlines([Str("margin=5pt")]),
            "header-includes": MetaList(
                [MetaBlocks([RawBlock(Format("tex"), "\\usepackage{multicol}")])]
            ),
        }
    ),
    content,
)
pandoc.write(doc, file=args.output[0], format="pdf", options=["--pdf-engine=xelatex"])
